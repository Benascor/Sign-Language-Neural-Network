{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition of American Sign Language via a Leap Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Matt Hehn, Ben Campbell, & Chris Cochran, December, 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In October 2017, we participated in Colorado State University's Virtual Reality Hackathon. During the 48-hour hackathon, our 4-person team developed a prototype Virtual Reality (VR) experience meant to allow people to learn or practice American Sign Language. Traditionally, learning ASL requires someone already skilled to act as a teacher/tutor to the new learner, which limits the ability of many to learn the language. Additionally, children having to learn ASL as their first language have yet greater difficulties learning the language without another way to convey meaning.\n",
    "\n",
    "Our project in the hackathon was designed to interpret sign language using a Leap Motion sensor mounted on the front of a VR headset, and provide active feedback to the user as to whether they had accurately mimicked a sign (and what it was, if they succeeed. A visual reference was shown in the VR environment). To aid in first-language learning, the system was designed to be able to enter a 'quiz' demo mode, which would ask the user to sign out a full word, at which point the simulation would spawn in a 3D model of the object the word refers to. We ultimately [won the hackathon](https://source.colostate.edu/sign-language-project-wins-second-annual-ram-hackathon/).\n",
    "\n",
    "The key limitation of this demo was its accuracy. In the original Unity project, the C# code used to take data from the sensor was making a very naive comparison with a threshhold of allowed difference between a single \"good\" reference for a sign and the current position of the hand and fingers. Using this method, we were only able to get the prototype to accurately and regularly recognize roughly six letters. \n",
    "\n",
    "It was our aim in this project (and even during the hackathon, though time limitations made it impossible) to train a neural network to recognize and categorize correct signs to then be re-incorporated back into the VR project at some later date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps I took.  Resources I used, such as our textbook [Russell and Norvig, 2014]. \n",
    "\n",
    "Say in detail what each team member did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all results.  Intermediate resultw might be shown in above Methods section.  Plots, tables, whatever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Russell and Norvig, 2014] Stuart Russell and Peter Norvig, [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/), Publisher. 2014.\n",
    "* [chuck] me, fldfkafjlak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Your report should contain approximately 1,500 to 5,000 words, in markdown cells.  You can count words by running the following python code in your report directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file Example of Project Report.ipynb is 123\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from nbformat import current\n",
    "import glob\n",
    "nbfile = glob.glob('*.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
